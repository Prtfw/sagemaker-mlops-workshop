{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tableau Amazon SageMaker - MLOps Workshop\n",
    "\n",
    "### Overview\n",
    "\n",
    "In this notebook you will automate an MLOps pipeline build, train, deploy and monitor an XGBoost regression model to automate the classification of unhappy customers for telecommunication service providers. The goal is to identify customers who may cancel their service soon so that you can entice them to stay. This is known as customer churn prediction.\n",
    "\n",
    "The dataset we use is publicly available and was mentioned in the book Discovering Knowledge in Data by Daniel T. Larose. It is attributed by the author to the University of California Irvine Repository of Machine Learning Datasets.\n",
    "\n",
    "This notebook will take you through a series of steps to execute the AWS CodePipeline stage as depicted below:\n",
    "\n",
    "1. [Data Prep / ETL Step](#Data-Prep)\n",
    "2. [Start Build](#Start-Build)\n",
    "3. [Wait for Training Job](#Wait-for-Training-Job)\n",
    "4. [Test Dev Deployment](#Test-Dev-Deployment)\n",
    "5. [Approve Prod Endpoint](#Approve-Prod-Deployment)\n",
    "6. [Test Prod Deployment](#Test-Prod-Deployment)\n",
    "7. [Model Monitoring](#Model-Monitoring)\n",
    "8. [CloudWatch Monitoring](#CloudWatch-Monitoring)\n",
    "9. [Retraining of the Model](#Retrain-Model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: pip in /opt/conda/lib/python3.7/site-packages (20.2.3)\n",
      "Name: sagemaker\n",
      "Version: 1.72.1\n",
      "Summary: Open source library for training and deploying models on Amazon SageMaker.\n",
      "Home-page: https://github.com/aws/sagemaker-python-sdk/\n",
      "Author: Amazon Web Services\n",
      "Author-email: None\n",
      "License: Apache License 2.0\n",
      "Location: /opt/conda/lib/python3.7/site-packages\n",
      "Requires: numpy, boto3, packaging, scipy, smdebug-rulesconfig, importlib-metadata, protobuf, protobuf3-to-dict\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "# # Import the latest sagemaker and boto3 SDKs\n",
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade pip\n",
    "#!{sys.executable} -m pip install -qU awscli boto3 \"sagemaker>=2.0.0\" tqdm\n",
    "!{sys.executable} -m pip install -qU awscli boto3 \"sagemaker>=1.71.0,<2.0.0\" tqdm\n",
    "!{sys.executable} -m pip show sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import logging\n",
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "from sagemaker import s3_input\n",
    "from sagemaker.s3 import S3Uploader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update the S3 Bucket information below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlops-customerchurn-artifact-us-east-2-325928439752\n"
     ]
    }
   ],
   "source": [
    "session = sagemaker.Session()\n",
    "bucket = \"mlops-customerchurn-artifact-us-east-2-325928439752\"  # Change this\n",
    "print(bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = boto3.Session().region_name\n",
    "id = uuid.uuid4().hex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_role = 'arn:aws:iam::325928439752:role/AWS-Glue-S3-Bucket-Access'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Create the AWS Glue Job¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = 'glue-customer-churn-etl-{}'.format(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = 'customerchurn'\n",
    "\n",
    "data_source = S3Uploader.upload(local_path='./data/customer-churn.csv',\n",
    "                               desired_s3_uri='s3://{}/{}'.format(bucket, project_name),\n",
    "                               session=session)\n",
    "\n",
    "train_prefix = 'train'\n",
    "val_prefix = 'validation'\n",
    "\n",
    "train_data = 's3://{}/{}/{}/'.format(bucket, project_name, train_prefix)\n",
    "validation_data = 's3://{}/{}/{}/'.format(bucket, project_name, val_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Create the AWS Glue Job¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_script_location = S3Uploader.upload(local_path='./code/glue_etl.py',\n",
    "                               desired_s3_uri='s3://{}/{}'.format(bucket, project_name),\n",
    "                               session=session)\n",
    "glue_client = boto3.client('glue')\n",
    "\n",
    "create_response = glue_client.create_job(\n",
    "    Name=job_name,\n",
    "    Description='PySpark job to extract the data and split in to training and validation data sets',\n",
    "    Role=glue_role, # you can pass your existing AWS Glue role here if you have used Glue before\n",
    "    ExecutionProperty={\n",
    "        'MaxConcurrentRuns': 2\n",
    "    },\n",
    "    Command={\n",
    "        'Name': 'glueetl',\n",
    "        'ScriptLocation': glue_script_location,\n",
    "        'PythonVersion': '3'\n",
    "    },\n",
    "    DefaultArguments={\n",
    "        '--job-language': 'python'\n",
    "    },\n",
    "    GlueVersion='1.0',\n",
    "    WorkerType='Standard',\n",
    "    NumberOfWorkers=2,\n",
    "    Timeout=60\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Glue Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_response = glue_client.start_job_run(JobName=create_response['Name'], \n",
    "                                        Arguments={\n",
    "                    '--BUCKET': bucket,                    \n",
    "                    '--S3_SOURCE': data_source,\n",
    "                    '--S3_DEST': 's3a://{}/{}/'.format(bucket, project_name),\n",
    "                    '--TRAIN_KEY': train_prefix + '/',\n",
    "                    '--VAL_KEY': val_prefix +'/'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = glue_client.get_job_run(JobName=create_response['Name'], RunId=run_response['JobRunId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (status['JobRun']['JobRunState'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Build\n",
    "\n",
    "Load variables from environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "region: us-east-2\n",
      "artifact bucket: mlops-customerchurn-artifact-us-east-2-325928439752\n",
      "pipeline: customerchurn\n",
      "model name: customerchurn\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import os\n",
    "import time\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "artifact_bucket = bucket #os.environ['ARTIFACT_BUCKET']\n",
    "pipeline_name = \"customerchurn\" #os.environ['PIPELINE_NAME']\n",
    "model_name = \"customerchurn\" #os.environ['MODEL_NAME']\n",
    "\n",
    "print('region: {}'.format(region))\n",
    "print('artifact bucket: {}'.format(artifact_bucket))\n",
    "print('pipeline: {}'.format(pipeline_name))\n",
    "print('model name: {}'.format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/Tableau/MLOps\n",
      "split train: 1866, val: 443, test: 24 \n"
     ]
    }
   ],
   "source": [
    "data_df = pd.read_csv('./data/training-dataset-with-header.csv')\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, val_df = train_test_split(data_df, test_size=0.20, random_state=42)\n",
    "val_df, test_df = train_test_split(val_df, test_size=0.05, random_state=42)\n",
    "\n",
    "# Set the index for our test dataframe\n",
    "test_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "print('split train: {}, val: {}, test: {} '.format(train_df.shape[0], val_df.shape[0], test_df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('./data/split/train.csv', index=False, header=False)\n",
    "val_df.to_csv('./data/split/validation.csv', index=False, header=False)\n",
    "\n",
    "# Save test and baseline with headers\n",
    "test_df.to_csv('./data/split/test.csv', index=False, header=True)\n",
    "train_df.to_csv('./data/split/baseline.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "# Get the session and default bucket\n",
    "session = sagemaker.session.Session()\n",
    "bucket = session.default_bucket()\n",
    "\n",
    "# Specify data previx version\n",
    "prefix = 'customerchurn'\n",
    "\n",
    "s3_train_uri = session.upload_data('data/train.csv', bucket, prefix + '/data/training')\n",
    "s3_val_uri = session.upload_data('data/validation.csv', bucket, prefix + '/data/validation')\n",
    "s3_baseline_uri = session.upload_data('data/baseline.csv', bucket, prefix + '/data/baseline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload data source meta data to trigger a new build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '31E34F11A107F173',\n",
       "  'HostId': 'ldwrrFD7x2uDFrYGUl+C1SRZVxpsYhvulXrAl+u+g6VjfC9Nc/o9DsVgOlQ5T3uIxuJ2fCiLSlI=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'ldwrrFD7x2uDFrYGUl+C1SRZVxpsYhvulXrAl+u+g6VjfC9Nc/o9DsVgOlQ5T3uIxuJ2fCiLSlI=',\n",
       "   'x-amz-request-id': '31E34F11A107F173',\n",
       "   'date': 'Wed, 07 Oct 2020 16:04:48 GMT',\n",
       "   'x-amz-version-id': 'FeAr07VfHLCCeaONisKJeqU.zGmvVOOp',\n",
       "   'etag': '\"cbea8cdbae4939488fd9b04afeb425ff\"',\n",
       "   'content-length': '0',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"cbea8cdbae4939488fd9b04afeb425ff\"',\n",
       " 'VersionId': 'FeAr07VfHLCCeaONisKJeqU.zGmvVOOp'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from io import BytesIO\n",
    "import zipfile\n",
    "import json\n",
    "\n",
    "input_data = {\n",
    "    'TrainingUri': s3_train_uri,\n",
    "    'ValidationUri': s3_val_uri,\n",
    "    'BaselineUri': s3_baseline_uri\n",
    "}\n",
    "\n",
    "hyperparameters = {\n",
    "    'num_round': 50\n",
    "}\n",
    "\n",
    "data_source_key = '{}/data-source.zip'.format(pipeline_name)\n",
    "\n",
    "zip_buffer = BytesIO()\n",
    "with zipfile.ZipFile(zip_buffer, 'a') as zf:\n",
    "    zf.writestr('inputData.json', json.dumps(input_data))\n",
    "    zf.writestr('hyperparameters.json', json.dumps(hyperparameters))\n",
    "zip_buffer.seek(0)\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "s3.put_object(Bucket=artifact_bucket, Key=data_source_key, Body=bytearray(zip_buffer.read()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wait for Training Job\n",
    "\n",
    "Follow the code pipeline to wait until the training job is complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a target=\"_blank\" href=\"https://us-east-2.console.aws.amazon.com/codesuite/codepipeline/pipelines/customerchurn/view?region=us-east-2\">Code Pipeline</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "\n",
    "HTML('<a target=\"_blank\" href=\"https://{0}.console.aws.amazon.com/codesuite/codepipeline/pipelines/{1}/view?region={0}\">Code Pipeline</a>'.format(region, pipeline_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we are waiting for the code pipeline to run, let's take a look at the model `run.py` code.  \n",
    "\n",
    "* We can see the XGBoost SageMaker estimator define in the `get_training_params` method.\n",
    "* The `training_uri` and  `validation_uri` are loaded from the `inputData.json` file in the data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtime\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mboto3\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msagemaker\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mworkflow\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mairflow\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m training_config\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mget_training_image\u001b[39;49;00m(region=\u001b[34mNone\u001b[39;49;00m):\n",
      "    region = region \u001b[35mor\u001b[39;49;00m boto3.Session().region_name\n",
      "    \u001b[34mreturn\u001b[39;49;00m sagemaker.image_uris.retrieve(\n",
      "        region=region, framework=\u001b[33m\"\u001b[39;49;00m\u001b[33mxgboost\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, version=\u001b[33m\"\u001b[39;49;00m\u001b[33m1.0-1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "    )\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mget_training_params\u001b[39;49;00m(\n",
      "    model_name,\n",
      "    job_id,\n",
      "    role,\n",
      "    image_uri,\n",
      "    training_uri,\n",
      "    validation_uri,\n",
      "    output_uri,\n",
      "    hyperparameters,\n",
      "    kms_key_id,\n",
      "):\n",
      "    \u001b[37m# Create the estimator\u001b[39;49;00m\n",
      "    xgb = sagemaker.estimator.Estimator(\n",
      "        image_uri,\n",
      "        role,\n",
      "        instance_count=\u001b[34m1\u001b[39;49;00m,\n",
      "        instance_type=\u001b[33m\"\u001b[39;49;00m\u001b[33mml.m4.xlarge\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        output_path=output_uri,\n",
      "    )\n",
      "    \u001b[37m# Set the hyperparameters overriding with any defaults\u001b[39;49;00m\n",
      "    params = {\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mmax_depth\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33m9\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33meta\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33m0.2\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mgamma\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33m4\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mmin_child_weight\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33m300\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33msubsample\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33m0.8\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mobjective\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mreg:linear\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mearly_stopping_rounds\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33m10\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mnum_round\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33m100\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    }\n",
      "    xgb.set_hyperparameters(**{**params, **hyperparameters})\n",
      "\n",
      "    \u001b[37m# Specify the data source\u001b[39;49;00m\n",
      "    s3_input_train = sagemaker.inputs.TrainingInput(\n",
      "        s3_data=training_uri, content_type=\u001b[33m\"\u001b[39;49;00m\u001b[33mcsv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "    )\n",
      "    s3_input_val = sagemaker.inputs.TrainingInput(\n",
      "        s3_data=validation_uri, content_type=\u001b[33m\"\u001b[39;49;00m\u001b[33mcsv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "    )\n",
      "    data = {\u001b[33m\"\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: s3_input_train, \u001b[33m\"\u001b[39;49;00m\u001b[33mvalidation\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: s3_input_val}\n",
      "\n",
      "    \u001b[37m# Get the training request\u001b[39;49;00m\n",
      "    request = training_config(xgb, inputs=data, job_name=job_id)\n",
      "    \u001b[34mreturn\u001b[39;49;00m {\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mParameters\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: {\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mModelName\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: model_name,\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mTrainJobId\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: job_id,\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mTrainJobRequest\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: json.dumps(request),\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mKmsKeyId\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: kms_key_id,\n",
      "        }\n",
      "    }\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mget_experiment\u001b[39;49;00m(model_name):\n",
      "    \u001b[34mreturn\u001b[39;49;00m {\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mExperimentName\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: model_name,\n",
      "    }\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mget_trial\u001b[39;49;00m(model_name, job_id):\n",
      "    \u001b[34mreturn\u001b[39;49;00m {\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mExperimentName\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: model_name,\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mTrialName\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: job_id,\n",
      "    }\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mget_suggest_baseline\u001b[39;49;00m(model_name, job_id, role, baseline_uri, kms_key_id):\n",
      "    \u001b[34mreturn\u001b[39;49;00m {\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mParameters\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: {\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mModelName\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: model_name,\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mTrainJobId\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: job_id,\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mMLOpsRoleArn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: role,\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mBaselineInputUri\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: baseline_uri,\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mKmsKeyId\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: kms_key_id,\n",
      "        }\n",
      "    }\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mget_dev_params\u001b[39;49;00m(model_name, job_id, role, image_uri, kms_key_id):\n",
      "    \u001b[34mreturn\u001b[39;49;00m {\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mParameters\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: {\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mImageRepoUri\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: image_uri,\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mModelName\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: model_name,\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mTrainJobId\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: job_id,\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mMLOpsRoleArn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: role,\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mVariantName\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mdev-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(model_name),\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mKmsKeyId\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: kms_key_id,\n",
      "        }\n",
      "    }\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mget_prd_params\u001b[39;49;00m(model_name, job_id, role, image_uri, kms_key_id):\n",
      "    dev_params = get_dev_params(model_name, job_id, role, image_uri, kms_key_id)[\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mParameters\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "    ]\n",
      "    prod_params = {\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mVariantName\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mprd-\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(model_name),\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mScheduleMetricName\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mfeature_baseline_drift_total_amount\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mScheduleMetricThreshold\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[36mstr\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33m0.20\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m),\n",
      "    }\n",
      "    \u001b[34mreturn\u001b[39;49;00m {\u001b[33m\"\u001b[39;49;00m\u001b[33mParameters\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[36mdict\u001b[39;49;00m(dev_params, **prod_params)}\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mget_pipeline_id\u001b[39;49;00m(pipeline_name):\n",
      "    \u001b[37m# Get pipeline execution id\u001b[39;49;00m\n",
      "    codepipeline = boto3.client(\u001b[33m\"\u001b[39;49;00m\u001b[33mcodepipeline\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    response = codepipeline.get_pipeline_state(name=pipeline_name)\n",
      "    \u001b[34mreturn\u001b[39;49;00m response[\u001b[33m\"\u001b[39;49;00m\u001b[33mstageStates\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m][\u001b[34m0\u001b[39;49;00m][\u001b[33m\"\u001b[39;49;00m\u001b[33mlatestExecution\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m][\u001b[33m\"\u001b[39;49;00m\u001b[33mpipelineExecutionId\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmain\u001b[39;49;00m(\n",
      "    pipeline_name,\n",
      "    model_name,\n",
      "    role,\n",
      "    data_bucket,\n",
      "    data_dir,\n",
      "    output_dir,\n",
      "    ecr_dir,\n",
      "    kms_key_id,\n",
      "):\n",
      "    \u001b[37m# Get the job id and source revisions\u001b[39;49;00m\n",
      "    job_id = get_pipeline_id(pipeline_name)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mjob id: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(job_id))\n",
      "    output_uri = \u001b[33m\"\u001b[39;49;00m\u001b[33ms3://\u001b[39;49;00m\u001b[33m{0}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{1}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(data_bucket, model_name)\n",
      "\n",
      "    \u001b[34mif\u001b[39;49;00m ecr_dir:\n",
      "        \u001b[37m# Load the image uri and input data config\u001b[39;49;00m\n",
      "        \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(os.path.join(ecr_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33mimageDetail.json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m), \u001b[33m\"\u001b[39;49;00m\u001b[33mr\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "            image_uri = json.load(f)[\u001b[33m\"\u001b[39;49;00m\u001b[33mImageURI\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "    \u001b[34melse\u001b[39;49;00m:\n",
      "        \u001b[37m# Get the the managed image uri for current region\u001b[39;49;00m\n",
      "        image_uri = get_training_image()\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mimage uri: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(image_uri))\n",
      "\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(os.path.join(data_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33minputData.json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m), \u001b[33m\"\u001b[39;49;00m\u001b[33mr\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "        input_data = json.load(f)\n",
      "        training_uri = input_data[\u001b[33m\"\u001b[39;49;00m\u001b[33mTrainingUri\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "        validation_uri = input_data[\u001b[33m\"\u001b[39;49;00m\u001b[33mValidationUri\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "        baseline_uri = input_data[\u001b[33m\"\u001b[39;49;00m\u001b[33mBaselineUri\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\n",
      "        \u001b[36mprint\u001b[39;49;00m(\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mtraining uri: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33mvalidation uri: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m baseline uri: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(\n",
      "                training_uri, validation_uri, baseline_uri\n",
      "            )\n",
      "        )\n",
      "\n",
      "    hyperparameters = {}\n",
      "    \u001b[34mif\u001b[39;49;00m os.path.exists(os.path.join(data_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33mhyperparameters.json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)):\n",
      "        \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(os.path.join(data_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33mhyperparameters.json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m), \u001b[33m\"\u001b[39;49;00m\u001b[33mr\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "            hyperparameters = json.load(f)\n",
      "            \u001b[34mfor\u001b[39;49;00m i \u001b[35min\u001b[39;49;00m hyperparameters:\n",
      "                hyperparameters[i] = \u001b[36mstr\u001b[39;49;00m(hyperparameters[i])\n",
      "\n",
      "    \u001b[37m# Create output directory\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m os.path.exists(output_dir):\n",
      "        os.mkdir(output_dir)\n",
      "\n",
      "    \u001b[37m# Write experiment and trial config\u001b[39;49;00m\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(os.path.join(output_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33mexperiment.json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m), \u001b[33m\"\u001b[39;49;00m\u001b[33mw\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "        config = get_experiment(model_name)\n",
      "        json.dump(config, f)\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(os.path.join(output_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33mtrial.json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m), \u001b[33m\"\u001b[39;49;00m\u001b[33mw\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "        config = get_trial(model_name, job_id)\n",
      "        json.dump(config, f)\n",
      "\n",
      "    \u001b[37m# Write the training request\u001b[39;49;00m\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(os.path.join(output_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33mtraining-job.json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m), \u001b[33m\"\u001b[39;49;00m\u001b[33mw\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "        params = get_training_params(\n",
      "            model_name,\n",
      "            job_id,\n",
      "            role,\n",
      "            image_uri,\n",
      "            training_uri,\n",
      "            validation_uri,\n",
      "            output_uri,\n",
      "            hyperparameters,\n",
      "            kms_key_id,\n",
      "        )\n",
      "        json.dump(params, f)\n",
      "\n",
      "    \u001b[37m# Write the baseline params for CFN\u001b[39;49;00m\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(os.path.join(output_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33msuggest-baseline.json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m), \u001b[33m\"\u001b[39;49;00m\u001b[33mw\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "        params = get_suggest_baseline(\n",
      "            model_name, job_id, role, baseline_uri, kms_key_id\n",
      "        )\n",
      "        json.dump(params, f)\n",
      "\n",
      "    \u001b[37m# Write the dev & prod params for CFN\u001b[39;49;00m\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(os.path.join(output_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33mdeploy-model-dev.json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m), \u001b[33m\"\u001b[39;49;00m\u001b[33mw\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "        params = get_dev_params(model_name, job_id, role, image_uri, kms_key_id)\n",
      "        json.dump(params, f)\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(os.path.join(output_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33mtemplate-model-prd.json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m), \u001b[33m\"\u001b[39;49;00m\u001b[33mw\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "        params = get_prd_params(model_name, job_id, role, image_uri, kms_key_id)\n",
      "        json.dump(params, f)\n",
      "\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "    parser = argparse.ArgumentParser(description=\u001b[33m\"\u001b[39;49;00m\u001b[33mLoad parameters\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--pipeline-name\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, required=\u001b[34mTrue\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--model-name\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, required=\u001b[34mTrue\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--role\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, required=\u001b[34mTrue\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--data-bucket\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, required=\u001b[34mTrue\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--data-dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, required=\u001b[34mTrue\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--output-dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, required=\u001b[34mTrue\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--ecr-dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, required=\u001b[34mFalse\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--kms-key-id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, required=\u001b[34mTrue\u001b[39;49;00m)\n",
      "    args = \u001b[36mvars\u001b[39;49;00m(parser.parse_args())\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33margs: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(args))\n",
      "    main(**args)\n"
     ]
    }
   ],
   "source": [
    "!pygmentize code/model/run.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the training and baseline job is complete we can inspect the exeriment metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TrialComponentName</th>\n",
       "      <th>DisplayName</th>\n",
       "      <th>SageMaker.InstanceType</th>\n",
       "      <th>train:rmse - Last</th>\n",
       "      <th>validation:rmse - Last</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mlops-customerchurn-edf18b2e-a503-4f60-9a7c-031c4c89add3-aws-training-job</td>\n",
       "      <td>Training</td>\n",
       "      <td>ml.m4.xlarge</td>\n",
       "      <td>0.32926</td>\n",
       "      <td>0.35421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mlops-customerchurn-pbl-edf18b2e-a503-4f60-9a7c-031c4c89add3-aws-processing-job</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>ml.m5.xlarge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                TrialComponentName  \\\n",
       "0        mlops-customerchurn-edf18b2e-a503-4f60-9a7c-031c4c89add3-aws-training-job   \n",
       "1  mlops-customerchurn-pbl-edf18b2e-a503-4f60-9a7c-031c4c89add3-aws-processing-job   \n",
       "\n",
       "  DisplayName SageMaker.InstanceType  train:rmse - Last  \\\n",
       "0    Training           ml.m4.xlarge            0.32926   \n",
       "1    Baseline           ml.m5.xlarge                NaN   \n",
       "\n",
       "   validation:rmse - Last  \n",
       "0                 0.35421  \n",
       "1                     NaN  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker import analytics\n",
    "model_analytics = analytics.ExperimentAnalytics(experiment_name=model_name)\n",
    "analytics_df = model_analytics.dataframe()\n",
    "\n",
    "if (analytics_df.shape[0] == 0):\n",
    "    raise(Exception('Please wait.  No training or baseline jobs'))\n",
    "\n",
    "pd.set_option('display.max_colwidth', 100) # Increase column width to show full copmontent name\n",
    "cols = ['TrialComponentName', 'DisplayName', 'SageMaker.InstanceType', \n",
    "        'train:rmse - Last', 'validation:rmse - Last'] # return the last rmse for training and validation\n",
    "analytics_df[analytics_df.columns & cols].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Dev Deployment\n",
    "\n",
    "One the endpoint has been deployed and awaiting approval, we can begin some tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "endpoint name: mlops-customerchurn-dev-edf18b2e-a503-4f60-9a7c-031c4c89add3\n"
     ]
    }
   ],
   "source": [
    "codepipeline = boto3.client('codepipeline')\n",
    "\n",
    "def get_pipeline_stage(pipeline_name, stage_name):\n",
    "    response = codepipeline.get_pipeline_state(name=pipeline_name)\n",
    "    for stage in response['stageStates']:\n",
    "        if stage['stageName'] == stage_name:\n",
    "            return stage\n",
    "        \n",
    "# Get last execution id\n",
    "deploy_dev = get_pipeline_stage(pipeline_name, 'DeployDev')\n",
    "if not 'latestExecution' in deploy_dev:\n",
    "    raise(Exception('Please wait.  Deploy dev not started'))\n",
    "    \n",
    "execution_id = deploy_dev['latestExecution']['pipelineExecutionId']\n",
    "dev_endpoint_name = 'mlops-{}-dev-{}'.format(model_name, execution_id)\n",
    "\n",
    "print('endpoint name: {}'.format(dev_endpoint_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait until the dev endpoint is in service (this can take up to 10 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint status: InService\n"
     ]
    }
   ],
   "source": [
    "sm = boto3.client('sagemaker')\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        response = sm.describe_endpoint(EndpointName=dev_endpoint_name)\n",
    "        print(\"Endpoint status: {}\".format(response['EndpointStatus']))\n",
    "        if response['EndpointStatus'] == 'InService':\n",
    "            break\n",
    "    except ClientError as e:\n",
    "        print(e.response[\"Error\"][\"Message\"])\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "try:\n",
    "    # Support SageMaker v2 SDK: https://sagemaker.readthedocs.io/en/stable/v2.html\n",
    "    from sagemaker.predictor import Predictor\n",
    "    from sagemaker.serializers import CSVSerializer\n",
    "    def get_predictor(endpoint_name):\n",
    "        xgb_predictor = Predictor(endpoint_name)\n",
    "        xgb_predictor.serializer = CSVSerializer()\n",
    "        return xgb_predictor\n",
    "except:\n",
    "    # Fallback to SageMaker v1.70 SDK\n",
    "    from sagemaker.predictor import RealTimePredictor, csv_serializer\n",
    "    def get_predictor(endpoint_name):\n",
    "        xgb_predictor = RealTimePredictor(endpoint_name)\n",
    "        xgb_predictor.content_type = 'text/csv'\n",
    "        xgb_predictor.serializer = csv_serializer\n",
    "        return xgb_predictor\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invoke the dev endpoint with test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.21538889408111572'\n",
      "b'0.11626008152961731'\n",
      "b'0.1974208950996399'\n",
      "b'0.11626008152961731'\n",
      "b'0.2893518805503845'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.17140549421310425'\n",
      "b'0.11626008152961731'\n",
      "b'0.2893518805503845'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.2893518805503845'\n",
      "b'0.21538889408111572'\n",
      "b'0.11626008152961731'\n",
      "b'0.21538889408111572'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.2893518805503845'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.21538889408111572'\n",
      "b'0.11626008152961731'\n",
      "b'0.21538889408111572'\n",
      "b'0.2893518805503845'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.21538889408111572'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.2893518805503845'\n",
      "b'0.21538889408111572'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.21538889408111572'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.2893518805503845'\n",
      "b'0.11626008152961731'\n",
      "b'0.2893518805503845'\n",
      "b'0.21538889408111572'\n",
      "b'0.11626008152961731'\n",
      "b'0.2893518805503845'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.2893518805503845'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.2893518805503845'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.21538889408111572'\n",
      "b'0.21538889408111572'\n",
      "b'0.2893518805503845'\n",
      "b'0.11626008152961731'\n",
      "b'0.21538889408111572'\n",
      "b'0.2893518805503845'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.21538889408111572'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.2893518805503845'\n",
      "b'0.11626008152961731'\n",
      "b'0.21538889408111572'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.21538889408111572'\n",
      "b'0.2893518805503845'\n",
      "b'0.2893518805503845'\n",
      "b'0.11626008152961731'\n",
      "b'0.2893518805503845'\n",
      "b'0.1974208950996399'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.2893518805503845'\n",
      "b'0.2893518805503845'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.2893518805503845'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.21538889408111572'\n",
      "b'0.11626008152961731'\n"
     ]
    }
   ],
   "source": [
    "dev_predictor = get_predictor(dev_endpoint_name)\n",
    "\n",
    "with open('./data/test_sample.csv', 'r') as f:\n",
    "    for row in f:\n",
    "        payload = row.rstrip('\\n')\n",
    "        predictions = dev_predictor.predict(data=payload)\n",
    "        print(predictions)\n",
    "        time.sleep(0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the response into a data frame, and join with predictions to calculate absolute error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approve Prod Deployment\n",
    "\n",
    "If we are happy with this metric, we can go ahead and approve with the widget below, or manually in the CodePipeline  by clicking the \"Review\" button.\n",
    "\n",
    "![Code pipeline](../docs/deploy-dev.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev approved: Approved\n"
     ]
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "def on_click(obj):\n",
    "    result = { 'summary': approval_text.value, 'status': obj.description }\n",
    "    response = codepipeline.put_approval_result(\n",
    "      pipelineName=pipeline_name,\n",
    "      stageName='DeployDev',\n",
    "      actionName='ApproveDeploy',\n",
    "      result=result,\n",
    "      token=approval_action['token']\n",
    "    )\n",
    "    button_box.close()\n",
    "    print(result)\n",
    "    \n",
    "# Create the widget if we are ready for approval\n",
    "deploy_dev = get_pipeline_stage(pipeline_name, 'DeployDev')\n",
    "if not 'latestExecution' in deploy_dev['actionStates'][-1]:\n",
    "    raise(Exception('Please wait.  Deploy dev not complete'))\n",
    "\n",
    "approval_action = deploy_dev['actionStates'][-1]['latestExecution']\n",
    "if approval_action['status'] == 'Succeeded':\n",
    "    print('Dev approved: {}'.format(approval_action['summary']))\n",
    "elif 'token' in approval_action:\n",
    "    approval_text = widgets.Text(placeholder='Optional approval message')   \n",
    "    approve_btn = widgets.Button(description=\"Approved\", button_style='success', icon='check')\n",
    "    reject_btn = widgets.Button(description=\"Rejected\", button_style='danger', icon='close')\n",
    "    approve_btn.on_click(on_click)\n",
    "    reject_btn.on_click(on_click)\n",
    "    button_box = widgets.HBox([approval_text, approve_btn, reject_btn])\n",
    "    display(button_box)\n",
    "else:\n",
    "    raise(Exception('Please wait.  No dev approval'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Prod Deployment\n",
    "\n",
    "The prod deployment will start shortly after approval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "deploy_prd = get_pipeline_stage(pipeline_name, 'DeployPrd')\n",
    "if not 'latestExecution' in deploy_prd or not 'latestExecution' in deploy_prd['actionStates'][0]:\n",
    "    raise(Exception('Please wait.  Deploy prd not started'))\n",
    "    \n",
    "execution_id = deploy_prd['latestExecution']['pipelineExecutionId']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Production deployment is managed through a CloudFormation stack which performs the following:\n",
    "\n",
    "1. Creates SageMaker Endpoint with Data Capture and AutoScaling enabled\n",
    "2. Creates Model Monitoring Schedule with CloudWatch Alarm\n",
    "3. Deploys an API Gateway Lambda with AWS Code Deploy\n",
    "\n",
    "![Code pipeline](../docs/cloud-formation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List the last events and how long ago they occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stack name: customerchurn-deploy-prd\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LogicalResourceId</th>\n",
       "      <th>ResourceStatus</th>\n",
       "      <th>ResourceStatusReason</th>\n",
       "      <th>TimeAgo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>customerchurn-deploy-prd</td>\n",
       "      <td>UPDATE_COMPLETE</td>\n",
       "      <td></td>\n",
       "      <td>00:15:59.249340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Model</td>\n",
       "      <td>DELETE_COMPLETE</td>\n",
       "      <td></td>\n",
       "      <td>00:15:59.485340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Model</td>\n",
       "      <td>DELETE_IN_PROGRESS</td>\n",
       "      <td></td>\n",
       "      <td>00:16:00.682340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EndpointConfig</td>\n",
       "      <td>DELETE_COMPLETE</td>\n",
       "      <td></td>\n",
       "      <td>00:16:01.274340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EndpointConfig</td>\n",
       "      <td>DELETE_IN_PROGRESS</td>\n",
       "      <td></td>\n",
       "      <td>00:16:02.716340</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          LogicalResourceId      ResourceStatus ResourceStatusReason  \\\n",
       "0  customerchurn-deploy-prd     UPDATE_COMPLETE                        \n",
       "1                     Model     DELETE_COMPLETE                        \n",
       "2                     Model  DELETE_IN_PROGRESS                        \n",
       "3            EndpointConfig     DELETE_COMPLETE                        \n",
       "4            EndpointConfig  DELETE_IN_PROGRESS                        \n",
       "\n",
       "          TimeAgo  \n",
       "0 00:15:59.249340  \n",
       "1 00:15:59.485340  \n",
       "2 00:16:00.682340  \n",
       "3 00:16:01.274340  \n",
       "4 00:16:02.716340  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from dateutil.tz import tzlocal\n",
    "\n",
    "def get_event_dataframe(events):\n",
    "    stack_cols = ['LogicalResourceId', 'ResourceStatus', 'ResourceStatusReason', 'Timestamp']\n",
    "    stack_event_df = pd.DataFrame(events)[stack_cols].fillna('')\n",
    "    stack_event_df['TimeAgo'] = (datetime.now(tzlocal())-stack_event_df['Timestamp'])\n",
    "    return stack_event_df.drop('Timestamp', axis=1)\n",
    "\n",
    "cfn = boto3.client('cloudformation')\n",
    "\n",
    "stack_name = stack_name='{}-deploy-prd'.format(pipeline_name)\n",
    "print('stack name: {}'.format(stack_name))\n",
    "\n",
    "# Get latest stack events\n",
    "while True:\n",
    "    try:\n",
    "        response = cfn.describe_stack_events(StackName=stack_name)\n",
    "        break\n",
    "    except ClientError as e:\n",
    "        print(e.response[\"Error\"][\"Message\"])\n",
    "    time.sleep(10)\n",
    "    \n",
    "get_event_dataframe(response['StackEvents']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can send some traffic to the production endpoint now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prod endpoint: mlops-customerchurn-prd-edf18b2e-a503-4f60-9a7c-031c4c89add3\n"
     ]
    }
   ],
   "source": [
    "prd_endpoint_name='mlops-{}-prd-{}'.format(model_name, execution_id)\n",
    "print('prod endpoint: {}'.format(prd_endpoint_name))\n",
    "\n",
    "#prd_endpoint_name=\"mlops-customerchurn-prd-6ad59d33-5fc9-4c55-a4c4-2b89d0398549\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait until the endpoint has finishing updated before we send some traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint status: InService\n"
     ]
    }
   ],
   "source": [
    "sm = boto3.client('sagemaker')\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        response = sm.describe_endpoint(EndpointName=prd_endpoint_name)\n",
    "        print(\"Endpoint status: {}\".format(response['EndpointStatus']))\n",
    "        # Wait until the endpoint is in service with data capture enabled\n",
    "        if response['EndpointStatus'] == 'InService' \\\n",
    "            and 'DataCaptureConfig' in response \\\n",
    "            and response['DataCaptureConfig']['EnableCapture']:\n",
    "            break\n",
    "    except ClientError as e:\n",
    "        print(e.response[\"Error\"][\"Message\"])\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send some inference to production endpoint now that data capture is enabled.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.21538889408111572'\n",
      "b'0.11626008152961731'\n",
      "b'0.1974208950996399'\n",
      "b'0.11626008152961731'\n",
      "b'0.2893518805503845'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.17140549421310425'\n",
      "b'0.11626008152961731'\n",
      "b'0.2893518805503845'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.2893518805503845'\n",
      "b'0.21538889408111572'\n",
      "b'0.11626008152961731'\n",
      "b'0.21538889408111572'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.2893518805503845'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.21538889408111572'\n",
      "b'0.11626008152961731'\n",
      "b'0.21538889408111572'\n",
      "b'0.2893518805503845'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.21538889408111572'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.2893518805503845'\n",
      "b'0.21538889408111572'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.21538889408111572'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.2893518805503845'\n",
      "b'0.11626008152961731'\n",
      "b'0.2893518805503845'\n",
      "b'0.21538889408111572'\n",
      "b'0.11626008152961731'\n",
      "b'0.2893518805503845'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.2893518805503845'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.2893518805503845'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.21538889408111572'\n",
      "b'0.21538889408111572'\n",
      "b'0.2893518805503845'\n",
      "b'0.11626008152961731'\n",
      "b'0.21538889408111572'\n",
      "b'0.2893518805503845'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.21538889408111572'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.2893518805503845'\n",
      "b'0.11626008152961731'\n",
      "b'0.21538889408111572'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.21538889408111572'\n",
      "b'0.2893518805503845'\n",
      "b'0.2893518805503845'\n",
      "b'0.11626008152961731'\n",
      "b'0.2893518805503845'\n",
      "b'0.1974208950996399'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.2893518805503845'\n",
      "b'0.2893518805503845'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.2893518805503845'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.11626008152961731'\n",
      "b'0.21538889408111572'\n",
      "b'0.11626008152961731'\n"
     ]
    }
   ],
   "source": [
    "prd_predictor = get_predictor(prd_endpoint_name)\n",
    "\n",
    "\n",
    "with open('./data/test_sample.csv', 'r') as f:\n",
    "    for row in f:\n",
    "        payload = row.rstrip('\\n')\n",
    "        predictions = prd_predictor.predict(data=payload)\n",
    "        print(predictions)\n",
    "        time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Rest API\n",
    "\n",
    "Get back the deployment progress and rest API endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stack status: UPDATE_COMPLETE\n",
      "deployment application: customerchurn-deploy-prd-ServerlessDeploymentApplication-1W3Q31G984RFW\n",
      "rest api: https://qmkg5uvca8.execute-api.us-east-2.amazonaws.com/Prod/api/\n"
     ]
    }
   ],
   "source": [
    "def get_stack_status(stack_name):\n",
    "    response = cfn.describe_stacks(StackName=stack_name)\n",
    "    if response['Stacks']:\n",
    "        stack = response['Stacks'][0]\n",
    "        outputs = None\n",
    "        if 'Outputs' in stack:\n",
    "            outputs = dict([(o['OutputKey'], o['OutputValue']) for o in stack['Outputs']])\n",
    "        return stack['StackStatus'], outputs \n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        status, outputs = get_stack_status(stack_name)\n",
    "        print('stack status: {}'.format(status))\n",
    "        if status.endswith('COMPLETE') or status.endswith('FAILED'):\n",
    "            break\n",
    "    except ClientError as e:\n",
    "        print(e.response[\"Error\"][\"Message\"])\n",
    "    time.sleep(10)    \n",
    "                \n",
    "\n",
    "if outputs:\n",
    "    print('deployment application: {}'.format(outputs['DeploymentApplication']))\n",
    "    print('rest api: {}'.format(outputs['RestApi']))                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the deployment application to see if its created and started to shift traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#HTML('<a target=\"_blank\" href=\"https://{0}.console.aws.amazon.com/codesuite/codedeploy/applications/{1}?region={0}\">Deployment Application</a>'.format(region, outputs['DeploymentApplication']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's ping the REST endpoint to see which SageMaker endpoint it is hitting.  Press STOP when deployment complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response code: 200: endpoint: mlops-customerchurn-prd-edf18b2e-a503-4f60-9a7c-031c4c89add3\n",
      "Deployment complete\n",
      "\n",
      "CPU times: user 14.8 ms, sys: 4.66 ms, total: 19.5 ms\n",
      "Wall time: 889 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from urllib import request\n",
    "\n",
    "headers = {\"Content-type\": \"text/csv\"}\n",
    "payload = test_df[test_df.columns[1:]].head(1).to_csv(header=False, index=False).encode('utf-8')\n",
    "rest_api = outputs['RestApi']\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        resp = request.urlopen(request.Request(rest_api, data=payload, headers=headers))\n",
    "        print(\"Response code: %d: endpoint: %s\" % (resp.getcode(), resp.getheader('x-sagemaker-endpoint')))\n",
    "        status, outputs = get_stack_status(stack_name) \n",
    "        if status.endswith('COMPLETE'):\n",
    "            print('Deployment complete\\n')\n",
    "            break\n",
    "    except ClientError as e:\n",
    "        print(e.response[\"Error\"][\"Message\"])\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Monitor\n",
    "\n",
    "Get the latest production deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "deploy_prd = get_pipeline_stage(pipeline_name, 'DeployPrd')\n",
    "if not 'latestExecution' in deploy_prd:\n",
    "    raise(Exception('Please wait.  Deploy prd not complete'))\n",
    "    \n",
    "execution_id = deploy_prd['latestExecution']['pipelineExecutionId']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline\n",
    "\n",
    "Load baseline processing job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing job name: mlops-customerchurn-pbl-edf18b2e-a503-4f60-9a7c-031c4c89add3\n",
      "schedule name: mlops-customerchurn-pms-edf18b2e-a503-4f60-9a7c-031c4c89add3\n"
     ]
    }
   ],
   "source": [
    "processing_job_name='mlops-{}-pbl-{}'.format(model_name, execution_id)\n",
    "schedule_name='mlops-{}-pms-{}'.format(model_name, execution_id)\n",
    "\n",
    "#schedule_name='mlops-{}-pms-{}'.format(model_name, \"6ad59d33-5fc9-4c55-a4c4-2b89d0398549\")\n",
    "print('processing job name: {}'.format(processing_job_name))\n",
    "print('schedule name: {}'.format(schedule_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.model_monitor import BaseliningJob, MonitoringExecution\n",
    "from sagemaker.s3 import S3Downloader\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "baseline_job = BaseliningJob.from_processing_name(sagemaker_session, processing_job_name)\n",
    "status = baseline_job.describe()['ProcessingJobStatus']\n",
    "if status != 'Completed':\n",
    "    raise(Exception('Please wait. Processing job not complete, status: {}'.format(status)))\n",
    "    \n",
    "baseline_results_uri  = baseline_job.outputs[0].destination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the generated constraints and statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'session' will be renamed to 'sagemaker_session' in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>inferred_type</th>\n",
       "      <th>numerical_statistics.common.num_present</th>\n",
       "      <th>numerical_statistics.common.num_missing</th>\n",
       "      <th>numerical_statistics.mean</th>\n",
       "      <th>numerical_statistics.sum</th>\n",
       "      <th>numerical_statistics.std_dev</th>\n",
       "      <th>numerical_statistics.min</th>\n",
       "      <th>numerical_statistics.max</th>\n",
       "      <th>numerical_statistics.distribution.kll.buckets</th>\n",
       "      <th>numerical_statistics.distribution.kll.sketch.parameters.c</th>\n",
       "      <th>numerical_statistics.distribution.kll.sketch.parameters.k</th>\n",
       "      <th>numerical_statistics.distribution.kll.sketch.data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Churn</td>\n",
       "      <td>Integral</td>\n",
       "      <td>2333</td>\n",
       "      <td>0</td>\n",
       "      <td>0.139306</td>\n",
       "      <td>325.0</td>\n",
       "      <td>0.346265</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[{'lower_bound': 0.0, 'upper_bound': 0.1, 'count': 2008.0}, {'lower_bound': 0.1, 'upper_bound': ...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Account Length</td>\n",
       "      <td>Integral</td>\n",
       "      <td>2333</td>\n",
       "      <td>0</td>\n",
       "      <td>101.276897</td>\n",
       "      <td>236279.0</td>\n",
       "      <td>39.552442</td>\n",
       "      <td>1.0</td>\n",
       "      <td>243.0</td>\n",
       "      <td>[{'lower_bound': 1.0, 'upper_bound': 25.2, 'count': 70.0}, {'lower_bound': 25.2, 'upper_bound': ...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[119.0, 100.0, 111.0, 181.0, 95.0, 104.0, 70.0, 120.0, 88.0, 111.0, 33.0, 106.0, 54.0, 87.0, 94...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VMail Message</td>\n",
       "      <td>Integral</td>\n",
       "      <td>2333</td>\n",
       "      <td>0</td>\n",
       "      <td>8.214316</td>\n",
       "      <td>19164.0</td>\n",
       "      <td>13.776908</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>[{'lower_bound': 0.0, 'upper_bound': 5.1, 'count': 1684.0}, {'lower_bound': 5.1, 'upper_bound': ...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[19.0, 0.0, 0.0, 40.0, 36.0, 0.0, 0.0, 24.0, 0.0, 0.0, 35.0, 0.0, 0.0, 0.0, 0.0, 41.0, 0.0, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Day Mins</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>2333</td>\n",
       "      <td>0</td>\n",
       "      <td>180.226489</td>\n",
       "      <td>420468.4</td>\n",
       "      <td>53.987179</td>\n",
       "      <td>0.0</td>\n",
       "      <td>350.8</td>\n",
       "      <td>[{'lower_bound': 0.0, 'upper_bound': 35.08, 'count': 14.0}, {'lower_bound': 35.08, 'upper_bound'...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[178.1, 160.3, 197.1, 105.2, 283.1, 113.6, 232.1, 212.7, 73.3, 176.9, 161.9, 128.6, 190.5, 223....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Day Calls</td>\n",
       "      <td>Integral</td>\n",
       "      <td>2333</td>\n",
       "      <td>0</td>\n",
       "      <td>100.259323</td>\n",
       "      <td>233905.0</td>\n",
       "      <td>20.165008</td>\n",
       "      <td>0.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>[{'lower_bound': 0.0, 'upper_bound': 16.5, 'count': 2.0}, {'lower_bound': 16.5, 'upper_bound': 3...</td>\n",
       "      <td>0.64</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>[[110.0, 138.0, 117.0, 61.0, 112.0, 87.0, 122.0, 73.0, 86.0, 128.0, 85.0, 83.0, 108.0, 109.0, 10...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             name inferred_type  numerical_statistics.common.num_present  \\\n",
       "0           Churn      Integral                                     2333   \n",
       "1  Account Length      Integral                                     2333   \n",
       "2   VMail Message      Integral                                     2333   \n",
       "3        Day Mins    Fractional                                     2333   \n",
       "4       Day Calls      Integral                                     2333   \n",
       "\n",
       "   numerical_statistics.common.num_missing  numerical_statistics.mean  \\\n",
       "0                                        0                   0.139306   \n",
       "1                                        0                 101.276897   \n",
       "2                                        0                   8.214316   \n",
       "3                                        0                 180.226489   \n",
       "4                                        0                 100.259323   \n",
       "\n",
       "   numerical_statistics.sum  numerical_statistics.std_dev  \\\n",
       "0                     325.0                      0.346265   \n",
       "1                  236279.0                     39.552442   \n",
       "2                   19164.0                     13.776908   \n",
       "3                  420468.4                     53.987179   \n",
       "4                  233905.0                     20.165008   \n",
       "\n",
       "   numerical_statistics.min  numerical_statistics.max  \\\n",
       "0                       0.0                       1.0   \n",
       "1                       1.0                     243.0   \n",
       "2                       0.0                      51.0   \n",
       "3                       0.0                     350.8   \n",
       "4                       0.0                     165.0   \n",
       "\n",
       "                                                         numerical_statistics.distribution.kll.buckets  \\\n",
       "0  [{'lower_bound': 0.0, 'upper_bound': 0.1, 'count': 2008.0}, {'lower_bound': 0.1, 'upper_bound': ...   \n",
       "1  [{'lower_bound': 1.0, 'upper_bound': 25.2, 'count': 70.0}, {'lower_bound': 25.2, 'upper_bound': ...   \n",
       "2  [{'lower_bound': 0.0, 'upper_bound': 5.1, 'count': 1684.0}, {'lower_bound': 5.1, 'upper_bound': ...   \n",
       "3  [{'lower_bound': 0.0, 'upper_bound': 35.08, 'count': 14.0}, {'lower_bound': 35.08, 'upper_bound'...   \n",
       "4  [{'lower_bound': 0.0, 'upper_bound': 16.5, 'count': 2.0}, {'lower_bound': 16.5, 'upper_bound': 3...   \n",
       "\n",
       "   numerical_statistics.distribution.kll.sketch.parameters.c  \\\n",
       "0                                                       0.64   \n",
       "1                                                       0.64   \n",
       "2                                                       0.64   \n",
       "3                                                       0.64   \n",
       "4                                                       0.64   \n",
       "\n",
       "   numerical_statistics.distribution.kll.sketch.parameters.k  \\\n",
       "0                                                     2048.0   \n",
       "1                                                     2048.0   \n",
       "2                                                     2048.0   \n",
       "3                                                     2048.0   \n",
       "4                                                     2048.0   \n",
       "\n",
       "                                                     numerical_statistics.distribution.kll.sketch.data  \n",
       "0  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0,...  \n",
       "1  [[119.0, 100.0, 111.0, 181.0, 95.0, 104.0, 70.0, 120.0, 88.0, 111.0, 33.0, 106.0, 54.0, 87.0, 94...  \n",
       "2  [[19.0, 0.0, 0.0, 40.0, 36.0, 0.0, 0.0, 24.0, 0.0, 0.0, 35.0, 0.0, 0.0, 0.0, 0.0, 41.0, 0.0, 0.0...  \n",
       "3  [[178.1, 160.3, 197.1, 105.2, 283.1, 113.6, 232.1, 212.7, 73.3, 176.9, 161.9, 128.6, 190.5, 223....  \n",
       "4  [[110.0, 138.0, 117.0, 61.0, 112.0, 87.0, 122.0, 73.0, 86.0, 128.0, 85.0, 83.0, 108.0, 109.0, 10...  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "baseline_statistics = baseline_job.baseline_statistics().body_dict\n",
    "schema_df = pd.json_normalize(baseline_statistics[\"features\"])\n",
    "schema_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'session' will be renamed to 'sagemaker_session' in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>inferred_type</th>\n",
       "      <th>completeness</th>\n",
       "      <th>num_constraints.is_non_negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Churn</td>\n",
       "      <td>Integral</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Account Length</td>\n",
       "      <td>Integral</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VMail Message</td>\n",
       "      <td>Integral</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Day Mins</td>\n",
       "      <td>Fractional</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Day Calls</td>\n",
       "      <td>Integral</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             name inferred_type  completeness  num_constraints.is_non_negative\n",
       "0           Churn      Integral           1.0                             True\n",
       "1  Account Length      Integral           1.0                             True\n",
       "2   VMail Message      Integral           1.0                             True\n",
       "3        Day Mins    Fractional           1.0                             True\n",
       "4       Day Calls      Integral           1.0                             True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_constraints = baseline_job.suggested_constraints().body_dict\n",
    "constraints_df = pd.json_normalize(baseline_constraints[\"features\"])\n",
    "constraints_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Data Capture\n",
    "\n",
    "Get the list of data capture files form the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 files\n",
      "\n",
      "Last file:\n",
      "{\n",
      "  \"captureData\": {\n",
      "    \"endpointInput\": {\n",
      "      \"observedContentType\": \"text/csv\",\n",
      "      \"mode\": \"INPUT\",\n",
      "      \"data\": \"22,0,110.3,107,166.5,93,202.3,96,9.5,5,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,1,0\",\n",
      "      \"encoding\": \"CSV\"\n",
      "    },\n",
      "    \"endpointOutput\": {\n",
      "      \"observedContentType\": \"text/csv; charset=utf-8\",\n",
      "      \"mode\": \"OUTPUT\",\n",
      "      \"data\": \"0.11626008152961731\",\n",
      "      \"encoding\": \"CSV\"\n",
      "    }\n",
      "  },\n",
      "  \"eventMetadata\": {\n",
      "    \"eventId\": \"2b785891-7b19-47bb-8eec-a12c45d4195c\",\n",
      "    \"inferenceTime\": \"2020-10-07T16:07:25Z\"\n",
      "  },\n",
      "  \"eventVersion\": \"0\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "bucket = sagemaker_session.default_bucket()\n",
    "data_capture_logs_uri = 's3://{}/{}/datacapture/{}'.format(bucket, model_name, prd_endpoint_name)\n",
    "\n",
    "capture_files = S3Downloader.list(data_capture_logs_uri)\n",
    "print('Found {} files'.format(len(capture_files)))\n",
    "\n",
    "if capture_files:\n",
    "    # Get the first line of the most recent file    \n",
    "    event = json.loads(S3Downloader.read_file(capture_files[-1]).split('\\n')[0])\n",
    "    print('\\nLast file:\\n{}'.format(json.dumps(event, indent=2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Monitoring Schedule\n",
    "\n",
    "The functions for plotting and rendering distribution statistics or constraint violations are implemented in a `utils` file so let's grab that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O utils.py --quiet https://raw.githubusercontent.com/awslabs/amazon-sagemaker-examples/master/sagemaker_model_monitor/visualization/utils.py\n",
    "import utils as mu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the schedule status, and when the next hourly run is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schedule Status: Scheduled\n",
      "Next schedule in 51 minutes\n"
     ]
    }
   ],
   "source": [
    "sm = boto3.client('sagemaker')\n",
    "\n",
    "response = sm.describe_monitoring_schedule(MonitoringScheduleName=schedule_name)\n",
    "print('Schedule Status: {}'.format(response['MonitoringScheduleStatus']))\n",
    "\n",
    "now = datetime.now(tzlocal())\n",
    "next_hour = (now+timedelta(hours=1)).replace(minute=0)\n",
    "scheduled_diff = (next_hour-now).seconds//60\n",
    "print('Next schedule in {} minutes'.format(scheduled_diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the latest completed monitoring schedule (which may have violations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schedule status: Failed, Created: 5 minutes ago\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Please wait.  No Schedules created",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-c7ca88bdf117>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0;32mraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Please wait.  No Schedules created'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Please wait.  No Schedules created"
     ]
    }
   ],
   "source": [
    "processing_job_arn = None\n",
    "\n",
    "while processing_job_arn == None:\n",
    "    try:\n",
    "        response = sm.list_monitoring_executions(MonitoringScheduleName=schedule_name)\n",
    "    except ClientError as e:\n",
    "        print(e.response[\"Error\"][\"Message\"])\n",
    "    for mon in response['MonitoringExecutionSummaries']:\n",
    "        status = mon['MonitoringExecutionStatus']\n",
    "        now = datetime.now(tzlocal())\n",
    "        created_diff = (now-mon['CreationTime']).seconds//60\n",
    "        print('Schedule status: {}, Created: {} minutes ago'.format(status, created_diff))\n",
    "        if status in ['Completed', 'CompletedWithViolations']:\n",
    "            processing_job_arn = mon['ProcessingJobArn']\n",
    "            break\n",
    "        if status == 'InProgress':\n",
    "            break\n",
    "    else:\n",
    "        raise(Exception('Please wait.  No Schedules created'))\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the monitoring execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-3e86ffa2df69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m execution = MonitoringExecution.from_processing_arn(sagemaker_session=sagemaker.Session(), \n\u001b[0;32m----> 2\u001b[0;31m                                                     processing_job_arn=processing_job_arn)\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mexec_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'InputName'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexecution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ProcessingInputs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mexec_results_uri\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdestination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/model_monitor/model_monitoring.py\u001b[0m in \u001b[0;36mfrom_processing_arn\u001b[0;34m(cls, sagemaker_session, processing_job_arn)\u001b[0m\n\u001b[1;32m   1970\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1971\u001b[0m         \"\"\"\n\u001b[0;32m-> 1972\u001b[0;31m         processing_job_name = processing_job_arn.split(\":\")[5][\n\u001b[0m\u001b[1;32m   1973\u001b[0m             \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"processing-job/\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1974\u001b[0m         ]  # This is necessary while the API only vends an arn.\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "execution = MonitoringExecution.from_processing_arn(sagemaker_session=sagemaker.Session(), \n",
    "                                                    processing_job_arn=processing_job_arn)\n",
    "exec_inputs = {inp['InputName']: inp for inp in execution.describe()['ProcessingInputs']}\n",
    "exec_results_uri = execution.output.destination\n",
    "\n",
    "print('Monitoring Execution results: {}'.format(exec_results_uri))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List the constraints, statistics and violations if they exist. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-18 19:10:01 aws-athena-query-results-325928439752-us-west-2\n",
      "2020-06-25 22:39:13 aws-codestar-us-west-2-325928439752\n",
      "2020-06-25 22:39:13 aws-codestar-us-west-2-325928439752-mlops-app\n",
      "2020-06-25 22:39:13 aws-codestar-us-west-2-325928439752-mlops-pipe\n",
      "2019-09-06 17:12:37 aws-deepracer-271c11d5-d2e5-4e3d-9570-2a0cd5688131\n",
      "2019-09-04 20:49:56 aws-glue-325928439752-us-west-2\n",
      "2019-07-18 18:18:37 aws-glue-scripts-325928439752-us-west-2\n",
      "2019-07-18 18:18:38 aws-glue-temporary-325928439752-us-west-2\n",
      "2020-05-18 17:29:42 aws-logs-325928439752-us-west-2\n",
      "2020-08-11 16:02:52 bulkqa-rbulkqas3bucket-7ad8tb0w7dvj\n",
      "2019-11-03 21:56:31 cdktoolkit-stagingbucket-1b4p98ozrvlxj\n",
      "2020-09-24 04:00:43 cf-templates-nzbdlmvwhhpo-us-east-2\n",
      "2020-03-30 17:37:15 cf-templates-nzbdlmvwhhpo-us-west-1\n",
      "2019-06-24 03:38:14 cf-templates-nzbdlmvwhhpo-us-west-2\n",
      "2020-07-01 09:26:16 cloudtrail-awslogs-325928439752-e0kcnfzn-isengard-do-not-delete\n",
      "2020-09-23 16:17:15 do-not-delete-gatedgarden-audit-325928439752\n",
      "2019-06-04 19:58:59 elasticbeanstalk-us-east-2-325928439752\n",
      "2020-06-27 07:40:10 elasticbeanstalk-us-west-2-325928439752\n",
      "2019-10-01 18:10:58 forecastdemopv\n",
      "2020-04-08 03:46:33 here-sagemaker-default\n",
      "2020-03-18 04:35:20 here-sagemaker-gt\n",
      "2020-04-21 04:36:08 here-sagemaker-hosting-issue-model\n",
      "2020-04-07 18:20:37 here-tech-dist-tf-mask-rcnn\n",
      "2020-04-08 21:12:00 here-tech-dist-tf-mask-rcnn-us-east-2\n",
      "2020-07-04 00:23:54 media-analysis-us-east-1-325928439752\n",
      "2020-10-01 16:34:23 mlops-customerchurn-artifact-us-east-2-325928439752\n",
      "2020-09-14 16:49:37 mlops-nyctaxi-artifact-us-east-1-325928439752\n",
      "2019-05-28 19:50:28 mlops-sagemaker-data\n",
      "2019-10-10 14:49:42 pv-capitalone-immersionday\n",
      "2020-02-09 03:28:48 pv-eks-ml-data\n",
      "2019-08-03 06:54:09 pv-fd-cc-results\n",
      "2019-08-03 06:54:10 pv-fd-cc-training-models\n",
      "2020-08-10 18:21:12 pv-gt-bulkqa\n",
      "2020-07-02 05:06:34 pv-here-algo-demo\n",
      "2020-03-05 15:57:20 pv-here-olp\n",
      "2020-04-04 16:31:22 pv-here-olp-uswest1\n",
      "2020-05-19 16:16:30 pv-inuit-gt\n",
      "2020-03-04 14:58:10 pv-kf-workshop-data\n",
      "2020-02-09 17:53:04 pv-kubeflow-pipeline-data\n",
      "2019-06-21 05:17:10 pv-mb3-cloudtrail-logs-storage\n",
      "2019-06-18 20:35:55 pv-mb3-sagemaker\n",
      "2019-07-30 20:50:11 pv-nd-exp\n",
      "2019-08-07 15:52:30 pv-nd-sagemaker-rcf\n",
      "2019-08-05 04:51:38 pv-nd-test1\n",
      "2019-08-01 18:59:59 pv-public-cfn\n",
      "2019-10-21 19:14:41 pv-redrover\n",
      "2019-05-28 15:53:36 pv-sagemaker\n",
      "2019-06-12 14:16:22 pv-sagemaker-gnw\n",
      "2020-09-17 05:17:43 pv-sagemaker-gt\n",
      "2019-10-01 18:47:39 pv-sagemaker-gt-dataset-imgclassification\n",
      "2020-09-17 05:34:32 pv-sagemaker-gt-use\n",
      "2019-08-07 13:27:40 pv-sagemaker-nd-traffic-training-models\n",
      "2020-04-19 21:43:27 pv-sfdc-kf-sagemaker-workshop\n",
      "2019-10-01 18:45:13 pv-sm-gt-dataset-imgclassification\n",
      "2020-07-17 00:29:43 pv-tab-workshop\n",
      "2020-09-24 03:54:48 pv-tableau-mlops-workshop\n",
      "2019-09-24 20:05:56 pv-textract-samples\n",
      "2019-05-21 16:20:43 pv-vscode-test\n",
      "2019-10-01 17:22:27 pvawspredmaint\n",
      "2020-07-07 02:58:58 pvmediaanalysis-mediaanalysiswebsitebucket-7nzeaov3s1nt\n",
      "2019-12-12 04:33:58 sagemaker-ap-south-1-325928439752\n",
      "2019-10-08 05:27:04 sagemaker-capone-forecast-useast1\n",
      "2019-10-31 18:47:45 sagemaker-forecast-uswest2\n",
      "2020-05-18 14:42:24 sagemaker-studio-325928439752-2o1fni4etcf\n",
      "2019-12-05 07:06:47 sagemaker-studio-325928439752-2q1twmplcoa\n",
      "2020-05-18 15:35:31 sagemaker-studio-325928439752-37vvm5p2lcf\n",
      "2020-05-18 19:38:25 sagemaker-studio-325928439752-ylgxnn6as4\n",
      "2020-09-01 06:35:40 sagemaker-studio-us-east-2-325928439752\n",
      "2020-05-18 20:05:14 sagemaker-studio-us-west-2-325928439752\n",
      "2019-10-10 18:23:15 sagemaker-us-east-1-325928439752\n",
      "2019-12-06 20:39:21 sagemaker-us-east-2-325928439752\n",
      "2019-06-05 23:05:50 sagemaker-us-west-2-325928439752\n",
      "2020-04-25 18:32:54 sfdc-kf-workshop-data\n",
      "2020-04-26 04:42:19 sfdc-kf-workshop-kubeflow-pipeline-data\n",
      "2019-10-23 02:37:35 stepfunctionssample-sagemak-bucketformodelanddata-1cibvvyqcqgq0\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $exec_results_uri/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'session' will be renamed to 'sagemaker_session' in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'execution' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-5bb979adb19f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Get the baseline and monitoring statistics & violations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbaseline_statistics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbaseline_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbaseline_statistics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mexecution_statistics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatistics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mviolations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstraint_violations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'violations'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'execution' is not defined"
     ]
    }
   ],
   "source": [
    "# Get the baseline and monitoring statistics & violations\n",
    "baseline_statistics = baseline_job.baseline_statistics().body_dict\n",
    "execution_statistics = execution.statistics().body_dict\n",
    "violations = execution.constraint_violations().body_dict['violations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'execution_statistics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-2755785c37ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m mu.show_violation_df(baseline_statistics=baseline_statistics, \n\u001b[0;32m----> 2\u001b[0;31m                      \u001b[0mlatest_statistics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexecution_statistics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m                      violations=violations)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'execution_statistics' is not defined"
     ]
    }
   ],
   "source": [
    "mu.show_violation_df(baseline_statistics=baseline_statistics, \n",
    "                     latest_statistics=execution_statistics, \n",
    "                     violations=violations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CloudWatch Monitoring\n",
    "\n",
    "AWS [CloudWatch Synthetics](https://aws.amazon.com/blogs/aws/new-use-cloudwatch-synthetics-to-monitor-sites-api-endpoints-web-workflows-and-more/) provides allow you to setup a canary to test that your API is returning an expected value on a regular interval.  This is a great way to validate that the blue/green deployment is not causing any downtime for our end-users.\n",
    "\n",
    "### Create Canary\n",
    "\n",
    "Let's setup a \"canary\" to continously test the production API, and a dashboard to visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating canary: mlops-customerchurn\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urlparse\n",
    "from string import Template\n",
    "from io import BytesIO\n",
    "import zipfile\n",
    "\n",
    "# Format the canary_js with rest_api and payload\n",
    "rest_url = urlparse(rest_api)\n",
    "\n",
    "with open('./notebook/canary.js') as f:\n",
    "    canary_js = Template(f.read()).substitute(hostname=rest_url.netloc, path=rest_url.path, \n",
    "                                              data=payload.decode('utf-8').strip())\n",
    "# Write the zip file\n",
    "zip_buffer = BytesIO()\n",
    "with zipfile.ZipFile(zip_buffer, 'w') as zf:\n",
    "    zip_path = 'nodejs/node_modules/apiCanaryBlueprint.js' # Set a valid path\n",
    "    zip_info = zipfile.ZipInfo(zip_path)\n",
    "    zip_info.external_attr = 0o0755 << 16 # Ensure the file is readable\n",
    "    zf.writestr(zip_info, canary_js)\n",
    "zip_buffer.seek(0)\n",
    "\n",
    "# Create the canary\n",
    "synth = boto3.client('synthetics')\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "s3_canary_uri = 's3://{}/{}'.format(artifact_bucket, model_name)\n",
    "canary_name = 'mlops-{}'.format(model_name)\n",
    "\n",
    "response = synth.create_canary(\n",
    "    Name=canary_name,\n",
    "    Code={\n",
    "        'ZipFile': bytearray(zip_buffer.read()),\n",
    "        'Handler': 'apiCanaryBlueprint.handler'\n",
    "    },\n",
    "    ArtifactS3Location=s3_canary_uri,\n",
    "    ExecutionRoleArn=role,\n",
    "    Schedule={ \n",
    "        'Expression': 'rate(10 minutes)', \n",
    "        'DurationInSeconds': 0 },\n",
    "    RunConfig={\n",
    "        'TimeoutInSeconds': 60,\n",
    "        'MemoryInMB': 960\n",
    "    },\n",
    "    SuccessRetentionPeriodInDays=31,\n",
    "    FailureRetentionPeriodInDays=31,\n",
    "    RuntimeVersion='syn-1.0',\n",
    ")\n",
    "\n",
    "print('Creating canary: {}'.format(canary_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the a CloudWatch alarm when success percent drops below 90% for that canary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating alarm: mlops-customerchurn-synth-lt-threshold\n"
     ]
    }
   ],
   "source": [
    "cloudwatch = boto3.client('cloudwatch')\n",
    "\n",
    "canary_alarm_name = '{}-synth-lt-threshold'.format(canary_name)\n",
    "\n",
    "response = cloudwatch.put_metric_alarm(\n",
    "    AlarmName=canary_alarm_name,\n",
    "    ComparisonOperator='LessThanThreshold',\n",
    "    EvaluationPeriods=1,\n",
    "    DatapointsToAlarm=1,\n",
    "    Period=600, # 10 minute interval\n",
    "    Statistic='Average',\n",
    "    Threshold=90.0,\n",
    "    ActionsEnabled=False,\n",
    "    AlarmDescription='SuccessPercent LessThanThreshold 90%',\n",
    "    Namespace='CloudWatchSynthetics',\n",
    "    MetricName='SuccessPercent',\n",
    "    Dimensions=[\n",
    "        {\n",
    "          'Name': 'CanaryName',\n",
    "          'Value': canary_name\n",
    "        },\n",
    "    ],\n",
    "    Unit='Seconds'\n",
    ")\n",
    "\n",
    "print('Creating alarm: {}'.format(canary_alarm_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's wait for the canary to be read, then start it and wait until running.  The"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Canary status: CREATING\n",
      "Canary status: READY\n",
      "Canary status: RUNNING\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a target=\"_blank\" href=\"https://us-east-2.console.aws.amazon.com/cloudwatch/home?region=us-east-2#synthetics:canary/detail/mlops-customerchurn\">CloudWatch Canary</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "while True:\n",
    "    try:\n",
    "        response = synth.get_canary(Name=canary_name)\n",
    "        status = response['Canary']['Status']['State']    \n",
    "        print('Canary status: {}'.format(status))\n",
    "        if status == 'ERROR':\n",
    "            raise(Exception(response['Canary']['Status']['StateReason']))    \n",
    "        elif status == 'READY':\n",
    "            synth.start_canary(Name=canary_name)\n",
    "        elif status == 'RUNNING':\n",
    "            break        \n",
    "    except ClientError as e:\n",
    "        if e.response[\"Error\"][\"Code\"] == \"ResourceNotFoundException\":\n",
    "            print('No canary found.')\n",
    "            break\n",
    "        print(e.response[\"Error\"][\"Message\"])\n",
    "    time.sleep(10)\n",
    "\n",
    "# Output a html link to the cloudwatch console\n",
    "HTML('<a target=\"_blank\" href=\"https://{0}.console.aws.amazon.com/cloudwatch/home?region={0}#synthetics:canary/detail/{1}\">CloudWatch Canary</a>'.format(region, canary_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dashboard\n",
    "\n",
    "Finally let's create a AWS CloudWatch Dashboard to visualize the key performane metrics and alarms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a target=\"_blank\" href=\"https://us-east-2.console.aws.amazon.com/cloudwatch/home?region=us-east-2#dashboards:name=mlops-customerchurn\">CloudWatch Dashboard</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sts = boto3.client('sts')\n",
    "account_id = sts.get_caller_identity().get('Account')\n",
    "dashboard_name = 'mlops-{}'.format(model_name)\n",
    "\n",
    "with open('./notebook/dashboard.json') as f:\n",
    "    dashboard_body = Template(f.read()).substitute(region=region, account_id=account_id, model_name=model_name)\n",
    "    response = cloudwatch.put_dashboard(\n",
    "        DashboardName=dashboard_name,\n",
    "        DashboardBody=dashboard_body\n",
    "    )\n",
    "\n",
    "# Output a html link to the cloudwatch dashboard\n",
    "HTML('<a target=\"_blank\" href=\"https://{0}.console.aws.amazon.com/cloudwatch/home?region={0}#dashboards:name={1}\">CloudWatch Dashboard</a>'.format(region, canary_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigger Retraining\n",
    "Our CodePipeline is configured with a [CloudWatch Events](https://docs.aws.amazon.com/codepipeline/latest/userguide/create-cloudtrail-S3-source.html) to start our pipeline for retraining when the drift detection metric alrams.\n",
    "\n",
    "We can simulate drift by putting metric `0.5` which is above the threshold of `0.2`.  This will trigger the alarm, and start the code pipeline retraining.\",\n",
    "Click through to the Alarm and CodePipeline with the links below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a target=\"_blank\" href=\"https://us-east-2.console.aws.amazon.com/cloudwatch/home?region=us-east-2#alarmsV2:alarm/mlops-nyctaxi-metric-gt-threshold\">CloudWatch Alarm</a> starts \n",
       "     <a target=\"_blank\" href=\"https://us-east-2.console.aws.amazon.com/codesuite/codepipeline/pipelines/customerchurn/view?region=us-east-2\">Code Pipeline</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Put a new metric to trigger an alaram\n",
    "response = cloudwatch.put_metric_data(\n",
    "    Namespace='aws/sagemaker/Endpoints/data-metrics',\n",
    "    MetricData=[\n",
    "        {\n",
    "            'MetricName': 'feature_baseline_drift_CustServ Calls',\n",
    "            'Dimensions': [\n",
    "                {\n",
    "                    'Name': 'MonitoringSchedule',\n",
    "                    'Value': schedule_name\n",
    "                },\n",
    "                {\n",
    "                    'Name': 'Endpoint',\n",
    "                    'Value': prd_endpoint_name\n",
    "                },\n",
    "            ],\n",
    "            'Timestamp': datetime.now(),\n",
    "            'Value': 0.5, # This is over the configured threshold of 0.2\n",
    "            'Unit': 'None'\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Output a html link to the cloudwatch dashboard\n",
    "alarm_name = 'mlops-nyctaxi-metric-gt-threshold'\n",
    "HTML('''<a target=\"_blank\" href=\"https://{0}.console.aws.amazon.com/cloudwatch/home?region={0}#alarmsV2:alarm/{1}\">CloudWatch Alarm</a> starts \n",
    "     <a target=\"_blank\" href=\"https://{0}.console.aws.amazon.com/codesuite/codepipeline/pipelines/{2}/view?region={0}\">Code Pipeline</a>'''.format(region, alarm_name, pipeline_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up\n",
    "\n",
    "First delete the stacks used as part of the pipeline for deployment, training job and suggest baseline.  For a model name of **customerchurn** that would be.\n",
    "\n",
    "* *customerchurn*-devploy-prd\n",
    "* *customerchurn*-devploy-dev\n",
    "* *customerchurn*-training-job\n",
    "* *customerchurn*-suggest-baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The follow code will stop and delete the canary you created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    try:\n",
    "        response = synth.get_canary(Name=canary_name)\n",
    "        status = response['Canary']['Status']['State']    \n",
    "        print('Canary status: {}'.format(status))\n",
    "        if status == 'ERROR':\n",
    "            raise(Exception(response['Canary']['Status']['StateReason']))    \n",
    "        elif status == 'STOPPED':\n",
    "            synth.delete_canary(Name=canary_name)\n",
    "        elif status == 'RUNNING':\n",
    "            synth.stop_canary(Name=canary_name)\n",
    "    except ClientError as e:\n",
    "        if e.response[\"Error\"][\"Code\"] == \"ResourceNotFoundException\":\n",
    "            print('Canary succesfully deleted.')\n",
    "            break\n",
    "        print(e.response[\"Error\"][\"Message\"])\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will delete the dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloudwatch.delete_alarms(AlarmNames=[canary_alarm_name])\n",
    "print('Alarm deleted')\n",
    "\n",
    "cloudwatch.delete_dashboards(DashboardNames=[dashboard_name])\n",
    "print('Dashboard deleted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally delete the stack you created for the AWS CodePipeline and Notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
